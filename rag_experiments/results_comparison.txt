╔═══════════════════════════════════════════════════════════════════════════════╗
║           CVE→CWE MATCHING: EXPERIMENTAL RESULTS COMPARISON                   ║
╚═══════════════════════════════════════════════════════════════════════════════╝

Dataset: 101,404 labeled CVE→CWE pairs
Evaluation: 100 random samples per experiment
Date: January 13, 2026

┌───────────────────────────────────────────────────────────────────────────────┐
│ PERFORMANCE METRICS                                                           │
└───────────────────────────────────────────────────────────────────────────────┘

Approach                    Top-k Recall    Top-1 Recall    LLM Final    Time
─────────────────────────────────────────────────────────────────────────────────
Baseline (TF-IDF)           57% (k=5)       33%            51%  ✓       207s
Phase 1 (Hybrid)            56% (k=5) ⬇️     -              48%  ⬇️      207s
Phase 2 (Abstraction)       58% (k=10)≈     21%  ⬇️⬇️        44%  ⬇️      325s


┌───────────────────────────────────────────────────────────────────────────────┐
│ VISUAL COMPARISON - TOP-K RECALL                                              │
└───────────────────────────────────────────────────────────────────────────────┘

 0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100%
 │────│────│────│────│────│────│────│────│────│────│────│
 
Baseline     ████████████████████████████████████████████████████░ 57%  ✓ BEST
Phase 1      ███████████████████████████████████████████████████   56%  ⬇️
Phase 2      ████████████████████████████████████████████████████░ 58%  ≈


┌───────────────────────────────────────────────────────────────────────────────┐
│ VISUAL COMPARISON - FINAL LLM ACCURACY                                        │
└───────────────────────────────────────────────────────────────────────────────┘

 0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100%
 │────│────│────│────│────│────│────│────│────│────│────│
 
Baseline     ███████████████████████████████████████████████░       51%  ✓ BEST
Phase 1      ████████████████████████████████████████████           48%  ⬇️
Phase 2      ████████████████████████████████████████                44%  ⬇️


┌───────────────────────────────────────────────────────────────────────────────┐
│ DETAILED BREAKDOWN                                                            │
└───────────────────────────────────────────────────────────────────────────────┘

BASELINE: TF-IDF + RAG + LLM
───────────────────────────────────────────────────────────────────────────────
Method:         TF-IDF vectors, cosine similarity, LLM selection
Complexity:     Low (simple bag-of-words)
Top-5 Recall:   57%
Top-1 Recall:   33%
LLM Accuracy:   51% (+18pp over top-1)
Time (100):     207 seconds
Verdict:        ✅ BEST - Simple and effective

Strengths:
  ✓ Direct keyword matching works well for technical terms
  ✓ CVE and CWE share security vocabulary ("SQL injection", "XSS")
  ✓ No training required, deterministic
  ✓ Fast inference

Limitations:
  ✗ Struggles with vocabulary gap (synonyms)
  ✗ No semantic understanding
  ✗ Can't exceed ~57% ceiling without domain training


PHASE 1: Hybrid Retrieval (BM25 + SBERT + RRF)
───────────────────────────────────────────────────────────────────────────────
Method:         BM25 (lexical) + SBERT (semantic) + Reciprocal Rank Fusion
Complexity:     High (3 retrievers, query expansion, fusion)
Top-5 Recall:   56% ⬇️ (-1pp)
LLM Accuracy:   48% ⬇️ (-3pp)
Time (100):     207 seconds
Verdict:        ❌ FAILED - Added complexity without benefit

Components:
  • BM25 retrieval (k1=1.5, b=0.75)
  • SBERT embeddings (all-mpnet-base-v2)
  • Reciprocal Rank Fusion (k=60)
  • Query expansion (50+ security synonyms)
  • CWE enrichment (extended descriptions)

Why it failed:
  ✗ Query expansion added noise (synonyms diluted signal)
  ✗ SBERT not trained for security domain
  ✗ BM25 parameters suboptimal for short queries/long docs
  ✗ RRF fusion diluted good signals
  ✗ Addressed wrong problem (algorithm vs representation)


PHASE 2: LLM Query Abstraction
───────────────────────────────────────────────────────────────────────────────
Method:         LLM abstracts CVE → generic pattern → TF-IDF retrieval
Complexity:     High (2 LLM calls per CVE)
Top-10 Recall:  58% ≈ (+1pp, not significant)
Top-1 Recall:   21% ⬇️⬇️ (-12pp)
LLM Accuracy:   44% ⬇️ (-7pp)
Time (100):     325 seconds (+57%)
Abstraction:    100% success rate (0 timeouts)
Verdict:        ❌ FAILED - Inconsistent, removed critical details

Pipeline:
  1. LLM Call 1: Abstract CVE description (60s timeout)
     "buffer overflow in libpng 1.2.3" → "out-of-bounds write"
  2. TF-IDF retrieval on abstracted query
  3. LLM Call 2: Select best CWE from top-10 (180s timeout)

Why it failed:
  ✗ Over-abstraction removed critical technical terms
  ✗ Inconsistent terminology (LLM ≠ CWE taxonomy)
  ✗ Lost "SQL injection" → matched wrong CWEs
  ✗ Top-1 dramatically worse (21% vs 33%)
  ✗ LLM doesn't know CWE-specific vocabulary
  ✗ Created NEW vocabulary gap instead of bridging old one


┌───────────────────────────────────────────────────────────────────────────────┐
│ ANALYSIS: WHY THE CEILING EXISTS                                              │
└───────────────────────────────────────────────────────────────────────────────┘

The 57% Ceiling - Why All Approaches Converge
───────────────────────────────────────────────────────────────────────────────

Problem 1: VOCABULARY GAP
  CVE:  "Buffer overflow in libpng 1.2.3 via crafted PNG file"
  CWE:  "Out-of-bounds Write" (CWE-787)
  ❌ Different words, same concept → No match

Problem 2: ABSTRACTION MISMATCH
  CVE:  Specific instance ("SQL injection in login.php")
  CWE:  Abstract pattern ("Improper Neutralization of SQL Commands")
  ❌ Different abstraction levels → Weak similarity

Problem 3: UNDER-SPECIFIED CVEs
  "Vulnerability allows remote code execution"
  ❌ Doesn't specify which weakness → Multiple CWEs plausible

Problem 4: CWE COMPLEXITY
  • 969 weakness types
  • Hierarchical (parents/children)
  • Overlapping definitions
  • Some hyper-specific (CWE-496: "Public Data Assigned to Private Array-Typed Field")
  ❌ Hard to distinguish similar CWEs

The ceiling isn't algorithmic—it's REPRESENTATIONAL
  → Need embeddings that understand CVE↔CWE vocabulary mapping
  → Only achievable through domain-specific training


┌───────────────────────────────────────────────────────────────────────────────┐
│ RECOMMENDATION: PATH FORWARD                                                  │
└───────────────────────────────────────────────────────────────────────────────┘

⭐ PRIMARY: Fine-Tune Embeddings on Labeled Data
───────────────────────────────────────────────────────────────────────────────

Why:        Only approach that addresses root cause (representation gap)
Data:       101,404 labeled CVE→CWE pairs (already available!)
Method:     Contrastive learning on bi-encoder (Sentence-BERT)
Expected:   75-85% top-5 recall (vs 57% now)
Effort:     2-3 hours GPU training, ~1 day dev
Hardware:   GPU required (A100, V100, or similar)

Training approach:
  • Positive pairs: CVE description ↔ matching CWE definition
  • Negative pairs: CVE description ↔ random CWE definitions
  • Loss: Contrastive loss or triplet loss
  • Base model: all-mpnet-base-v2 or similar
  • Epochs: 3-5

Result: Embeddings that KNOW:
  ✓ "buffer overflow" ≈ "out-of-bounds write"
  ✓ "SQL injection" ≈ "improper neutralization of SQL commands"
  ✓ CVE-specific terminology maps to CWE taxonomy


Alternative Approaches (If Fine-Tuning Not Available)
───────────────────────────────────────────────────────────────────────────────

Option 2: Cross-Encoder Re-Ranker
  • Binary classifier: "Does CVE match this CWE?"
  • Train on positive/negative pairs
  • Use on top-20 candidates
  • Pro: More accurate, no GPU needed for inference
  • Con: Slower (must score each candidate)

Option 3: Large LLM Few-Shot
  • Use GPT-4 / Claude with examples
  • Include CVE→CWE pairs in prompt
  • Pro: No training, potentially very accurate
  • Con: API costs, rate limits, external dependency

Option 4: Stay with Baseline
  • 51% accuracy acceptable for many use cases
  • Simple, fast, reliable
  • Can augment with human review for critical cases


NOT Recommended:
───────────────────────────────────────────────────────────────────────────────
  ❌ More retrieval fusion methods
  ❌ Query expansion approaches
  ❌ LLM abstraction/rewriting
  ❌ Traditional NLP preprocessing

Reason: All tested approaches fail to break the 57% ceiling because they don't
        address the fundamental representation problem.


┌───────────────────────────────────────────────────────────────────────────────┐
│ CONCLUSION                                                                    │
└───────────────────────────────────────────────────────────────────────────────┘

Key Findings:
─────────────
1. Simple TF-IDF baseline BEATS all advanced methods (51% final accuracy)
2. The 57% ceiling is a REPRESENTATION problem, not an algorithm problem
3. General-purpose semantic models (SBERT) don't help without domain training
4. LLM abstraction removes critical information instead of bridging gaps

Bottom Line:
────────────
Current best:  Baseline TF-IDF + LLM = 51% accuracy
To exceed 60%: Fine-tune embeddings on 101K labeled pairs
Expected:      75-85% accuracy with domain-specific training

The path forward is clear: invest in fine-tuning embeddings, not algorithm tweaking.

╔═══════════════════════════════════════════════════════════════════════════════╗
║  Report generated: January 13, 2026                                           ║
║  For details: CVE_CWE_MATCHING_EXPERIMENTS_REPORT.md                          ║
╚═══════════════════════════════════════════════════════════════════════════════╝
